{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "#Q Table 초기화(2차원)\n",
    "# Q: 주어진 state에서 어떤 action을 취할 것인지에 대한 길잡이\n",
    "# env.observation_space.n: 환경의 경우의 수\n",
    "# env.action_space.n: 행동의 경우의 수\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 초기화\n",
    "# 할인률\n",
    "dis = 0.99 \n",
    "\n",
    "# 시도 횟수(에피소드)\n",
    "num_episodes = 2000\n",
    "\n",
    "# 에피소드마다 총 보상의 합을 저장하는 리스트\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q업데이트 - 랜덤방식\n",
    "frames=[]\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    # Q learning 알고리즘\n",
    "    while not done:\n",
    "        # Action 중 가장 R이 큰 Action을 랜덤으로 고르는 방식\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n)/(i+1))\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Q = R + 할인율*max(Q)\n",
    "        Q[state, action] = reward + dis*np.max(Q[new_state, :])\n",
    "        \n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "\n",
    "        # 애니메이션을 위하여 정보 기록\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': new_state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )  \n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.019\n",
      "Final Q-Table Values\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.96059601 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.970299   0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q업데이트 - E-greedy 방식\n",
    "frames=[]\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    # exploration의 확률\n",
    "    e = 1. / ((i/100) + 1)\n",
    "\n",
    "    # Q learning 알고리즘\n",
    "    while not done:\n",
    "        # E-Greedy 알고리즘으로 Action을 고르는 방식\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Q = R + 할인율*max(Q)\n",
    "        Q[state, action] = reward + dis*np.max(Q[new_state, :])\n",
    "\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "\n",
    "        # 애니메이션을 위하여 정보 기록\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': new_state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 0.051\n",
      "Final Q-Table Values\n",
      "[[0.0643292  0.0643292  0.0643292  0.62352539]\n",
      " [0.         0.         0.         0.06241855]\n",
      " [0.         0.         0.         0.06497899]\n",
      " [0.         0.         0.         0.0643292 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.9801     0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.12741334 1.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Timestep: 250\n",
      "State: 0\n",
      "Action: 1\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
